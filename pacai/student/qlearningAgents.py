import random
import collections
from pacai.agents.learning.reinforcement import ReinforcementAgent
from pacai.util import probability
from pacai.util import reflection

class QLearningAgent(ReinforcementAgent):
    """
    A Q-Learning agent.

    Some functions that may be useful:

    `pacai.agents.learning.reinforcement.ReinforcementAgent.getAlpha`:
    Get the learning rate.

    `pacai.agents.learning.reinforcement.ReinforcementAgent.getDiscountRate`:
    Get the discount rate.

    `pacai.agents.learning.reinforcement.ReinforcementAgent.getEpsilon`:
    Get the exploration probability.

    `pacai.agents.learning.reinforcement.ReinforcementAgent.getLegalActions`:
    Get the legal actions for a reinforcement agent.

    `pacai.util.probability.flipCoin`:
    Flip a coin (get a binary value) with some probability.

    `random.choice`:
    Pick randomly from a list.

    Additional methods to implement:

    `pacai.agents.base.BaseAgent.getAction`:
    Compute the action to take in the current state.
    With probability `pacai.agents.learning.reinforcement.ReinforcementAgent.getEpsilon`,
    we should take a random action and take the best policy action otherwise.
    Note that if there are no legal actions, which is the case at the terminal state,
    you should choose None as the action.

    `pacai.agents.learning.reinforcement.ReinforcementAgent.update`:
    The parent class calls this to observe a state transition and reward.
    You should do your Q-Value update here.
    Note that you should never call this function, it will be called on your behalf.

    DESCRIPTION: QL Agent designed to be able to update it's q-values in real time
    in relation to previous experiences with the related state spaces and action sequences.
    The agent starts with an empty dictionary that will hold known q-values generated by
    state-action pairs. Depending on it's intent to explore, the agent has a chance to either
    conform to its current, learned policy (built over it's sequence of actions) or explore
    in a random action to one of the surrounding states irregardless of the policy evaluation.
    """

    def __init__(self, index, **kwargs):
        super().__init__(index, **kwargs)

        # You can initialize Q-values here.
        self.QValues = {}

    def getQValue(self, state, action):
        """
        Get the Q-Value for a `pacai.core.gamestate.AbstractGameState`
        and `pacai.core.directions.Directions`.
        Should return 0.0 if the (state, action) pair has never been seen.
        """
        return self.QValues[state, action] if (state, action) in self.QValues else 0.0

    def getValue(self, state):
        """
        Return the value of the best action in a state.
        I.E., the value of the action that solves: `max_action Q(state, action)`.
        Where the max is over legal actions.
        Note that if there are no legal actions, which is the case at the terminal state,
        you should return a value of 0.0.

        This method pairs with `QLearningAgent.getPolicy`,
        which returns the actual best action.
        Whereas this method returns the value of the best action.
        """
        actions = self.getLegalActions(state)
        values = [self.getQValue(state, action) for action in actions]
        return max(values) if values else 0.0

    def getPolicy(self, state):
        """
        Return the best action in a state.
        I.E., the action that solves: `max_action Q(state, action)`.
        Where the max is over legal actions.
        Note that if there are no legal actions, which is the case at the terminal state,
        you should return a value of None.

        This method pairs with `QLearningAgent.getValue`,
        which returns the value of the best action.
        Whereas this method returns the best action itself.
        """
        actions = self.getLegalActions(state)
        maxValue = self.getValue(state)
        for action in actions:
            if maxValue == self.getQValue(state, action):
                return action
        return None

    def getAction(self, state):
        actions = self.getLegalActions(state)
        action = \
            random.choice(actions) if probability.flipCoin(self.epsilon) else self.getPolicy(state)
        return action

    def update(self, state, action, nextState, reward):
        sample = reward + (self.discountRate * self.getValue(nextState))
        updatedQ = (1 - self.alpha) * self.getQValue(state, action)  # new q-value
        updatedQ += self.alpha * sample
        self.QValues[state, action] = updatedQ

class PacmanQAgent(QLearningAgent):
    """
    Exactly the same as `QLearningAgent`, but with different default parameters.
    """

    def __init__(self, index, epsilon = 0.05, gamma = 0.8, alpha = 0.2, numTraining = 0, **kwargs):
        kwargs['epsilon'] = epsilon
        kwargs['gamma'] = gamma
        kwargs['alpha'] = alpha
        kwargs['numTraining'] = numTraining

        super().__init__(index, **kwargs)

    def getAction(self, state):
        """
        Simply calls the super getAction method and then informs the parent of an action for Pacman.
        Do not change or remove this method.
        """

        action = super().getAction(state)
        self.doAction(state, action)

        return action

class ApproximateQAgent(PacmanQAgent):
    """
    An approximate Q-learning agent.

    You should only have to overwrite `QLearningAgent.getQValue`
    and `pacai.agents.learning.reinforcement.ReinforcementAgent.update`.
    All other `QLearningAgent` functions should work as is.

    Additional methods to implement:

    `QLearningAgent.getQValue`:
    Should return `Q(state, action) = w * featureVector`,
    where `*` is the dotProduct operator.

    `pacai.agents.learning.reinforcement.ReinforcementAgent.update`:
    Should update your weights based on transition.

    DESCRIPTION: update function reallocates weight IRT correction from previous
    pass. getQValue takes features present for the specific state-action pair
    and takes the dot product of the weights and the feature.
    """

    def __init__(self, index,
            extractor = 'pacai.core.featureExtractors.IdentityExtractor', **kwargs):
        super().__init__(index, **kwargs)
        self.featExtractor = reflection.qualifiedImport(extractor)

        # You might want to initialize weights here.
        self.weights = collections.Counter()

    def getQValue(self, state, action):
        features = self.featExtractor.getFeatures(self, state, action)
        qValue = 0.0
        for f in features.keys():
            qValue += self.weights[f] * features[f]  # dot product
        return qValue

    def update(self, state, action, nextState, reward):  # w_i = w_i + a[correction]f_i(s,a)
        features = self.featExtractor.getFeatures(self, state, action)
        correction = \
            (reward + (self.discountRate * self.getValue(nextState))) \
            - self.getQValue(state, action)
        for f in features.keys():
            self.weights[f] += self.alpha * correction * features[f]

    def final(self, state):
        """
        Called at the end of each game.
        """

        # Call the super-class final method.
        super().final(state)

        # Did we finish training?
        if self.episodesSoFar == self.numTraining:
            # You might want to print your weights here for debugging.
            # *** Your Code Here ***
            pass
